{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ece210-d044-4a73-bd47-8268963e6f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def computeBleu(prediction, label):\n",
    "    smoothingFunction = SmoothingFunction().method1 #apply smoothing function to avoid nullifying the value\n",
    "    #dynamically adjust the weight according to the length of the text sequence\n",
    "    if len(label) == 1:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(1,), smoothing_function=smoothingFunction)\n",
    "    elif len(label) == 2:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(0.5,0.5), smoothing_function=smoothingFunction)\n",
    "    elif len(label) == 3:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(1/3,1/3,1/3), smoothing_function=smoothingFunction)\n",
    "    else:\n",
    "        return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothingFunction)\n",
    "    \n",
    "\n",
    "def computeRouge(references, candidates):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidates, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def computeBleu1(prediction, label):\n",
    "    prediction = prediction.replace(\"</s>\", \"\") #remove unwanted tags\n",
    "    prediction = prediction.lower().split() #convert prediction to lower case\n",
    "    label = label.lower().split() #convert label to lower case \n",
    "    smoothingFunction = SmoothingFunction().method1 #apply smoothing function to avoid nullifying the value\n",
    "    return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(1,0,0,0), smoothing_function=smoothingFunction) #bleu1 focuses on unigrams, so adjust the weight accordingly\n",
    "\n",
    "\n",
    "def computeBleu2(prediction, label):\n",
    "    prediction = prediction.replace(\"</s>\", \"\") #remove unwanted tags\n",
    "    prediction = prediction.lower().split() #convert prediction to lower case\n",
    "    label = label.lower().split() #convert label to lower case \n",
    "    smoothingFunction = SmoothingFunction().method1 #apply smoothing function to avoid nullifying the value\n",
    "    return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(0.5,0.5,0,0), smoothing_function=smoothingFunction) #bleu2 focuses on bigrams, so adjust the weight accordingly\n",
    "\n",
    "\n",
    "def computeBleu3(prediction, label):\n",
    "    prediction = prediction.replace(\"</s>\", \"\")\n",
    "    prediction = prediction.lower().split()\n",
    "    label = label.lower().split()\n",
    "    smoothingFunction = SmoothingFunction().method1\n",
    "    return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(1/3,1/3,1/3), smoothing_function=smoothingFunction) #bleu3 focuses on 3-grams, so adjust the weight accordingly\n",
    "\n",
    "\n",
    "def computeBleu4(prediction, label):\n",
    "    prediction = prediction.replace(\"</s>\", \"\")\n",
    "    prediction = prediction.lower().split()\n",
    "    label = label.lower().split()\n",
    "    smoothingFunction = SmoothingFunction().method1\n",
    "    return nltk.translate.bleu_score.sentence_bleu([label], prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothingFunction) #bleu4 focuses on 4-grams, so adjust the weight accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e15c925-93ae-4bee-a9a2-3beac7e4e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file: #use the predictions from the models \n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/BleuScore.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions']) #since the csv reader reads a list as a string, this function is needed to convert the string back to a list\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\") #remove unwanted characters\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\") #remove double whitespaces\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu(pred, row['Labels'].lower()) #compute the score for each sequence in the 4 return sequences\n",
    "                if bleuScore < bleuTemp: #the best score will be assigned from the 4 scores\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5f5017-6279-4dd3-aed3-4aa6b85ddbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu1Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu1(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1015b420-12dc-463b-ad77-f51160f2f909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu2Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = 0\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu2(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8697501a-beba-42fc-8390-a357ea77153f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu3Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu3(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2619ba7b-51f0-4480-bf40-760341cac9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu4Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu4(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dca06d5-4c34-45c4-9279-c8602a20ff97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Rouge1ScoreF1.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'RougeScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            rougeScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                if pred == \"\":\n",
    "                    pred = \"$\"\n",
    "                rougeTemp = computeRouge(pred, row['Labels'].lower())\n",
    "                if rougeScore < rougeTemp['rouge-1']['f']:\n",
    "                    rougeScore = rougeTemp['rouge-1']['f']\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'RougeScore': rougeScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb0b8046-f13d-4719-8c64-66a790e6d682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Rouge2ScoreF1.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'RougeScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            rougeScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                if pred == \"\":\n",
    "                    pred = \"$\"\n",
    "                rougeTemp = computeRouge(pred, row['Labels'].lower())\n",
    "                if rougeScore < rougeTemp['rouge-2']['f']:\n",
    "                    rougeScore = rougeTemp['rouge-2']['f']\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'RougeScore': rougeScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b172ce12-18f0-4576-a057-9e49ae1e9f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"Predictions and Labels.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/RougeLScoreF1.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'RougeScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            rougeScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                if pred == \"\":\n",
    "                    pred = \"$\"\n",
    "                rougeTemp = computeRouge(pred, row['Labels'].lower())\n",
    "                if rougeScore < rougeTemp['rouge-l']['f']:\n",
    "                    rougeScore = rougeTemp['rouge-l']['f']\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'RougeScore': rougeScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa49e1",
   "metadata": {},
   "source": [
    "## Below is the evaluation for the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a681e586-7297-4ac6-9762-3383b3d0b837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/BleuScore.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37588a9f-c801-4a6c-8d10-ccbac482c170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu1Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu1(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99720bef-897a-4299-8120-383b8d0256d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu2Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu2(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fa213e5-1552-4657-ad64-07a96ffb6ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu3Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu3(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11596272-627e-408e-8761-6f1115fdc8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu4Score.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            bleuScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                bleuTemp = computeBleu4(pred, row['Labels'].lower())\n",
    "                if bleuScore < bleuTemp:\n",
    "                    bleuScore = bleuTemp\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'BleuScore': bleuScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "597c99bb-3e72-4227-81f7-17a1abf5396d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Rouge1ScoreF1.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'RougeScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            rougeScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                rougeTemp = computeRouge(pred, row['Labels'].lower())\n",
    "                if rougeScore < rougeTemp['rouge-1']['f']:\n",
    "                    rougeScore = rougeTemp['rouge-1']['f']\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'RougeScore': rougeScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b444149-680d-4123-8753-281266b0890b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Rouge2ScoreF1.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'RougeScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            rougeScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                rougeTemp = computeRouge(pred, row['Labels'].lower())\n",
    "                if rougeScore < rougeTemp['rouge-2']['f']:\n",
    "                    rougeScore = rougeTemp['rouge-2']['f']\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'RougeScore': rougeScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea48f43e-2cb5-4ac0-92be-688e83a09940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"Predictions and Labels(Base).csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/RougeLScoreF1.csv\", 'w', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'RougeScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        iterator = 0\n",
    "        for row in csvfile:\n",
    "            rougeScore = -1\n",
    "            finalPred = \"\"\n",
    "            preds = ast.literal_eval(row['Predictions'])\n",
    "            for pred in preds:\n",
    "                pred = pred.replace(\"⁇\", \"\")\n",
    "                pred = pred.replace(\"</s>\", \"\")\n",
    "                pred = pred.replace(\"  \", \"\")\n",
    "                pred = pred.replace(\".\", \"\")\n",
    "                rougeTemp = computeRouge(pred, row['Labels'].lower())\n",
    "                if rougeScore < rougeTemp['rouge-l']['f']:\n",
    "                    rougeScore = rougeTemp['rouge-l']['f']\n",
    "                    finalPred = pred\n",
    "            writer.writerow({'Prediction': finalPred, 'Labels': row['Labels'].lower(), 'RougeScore': rougeScore})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f2ac6",
   "metadata": {},
   "source": [
    "## The code below is used to compute the average scores for ROUGE and BLEU scores for the fine-tuned model and the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b6bffb2-a93d-40af-b456-965497958094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Rouge/RougeLScoreF1.csv\", 'r') as file: #open the file with the rouge scores\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Rouge/RougeLScoreF1.csv\", 'a', newline='') as writefile: #append to the same file\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        # the code below computes the average score\n",
    "        for row in csvfile:\n",
    "            total += float(row['RougeScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'RougeScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "093a5dea-e3cf-44ed-8f4a-f912bb9759bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Rouge/Rouge2ScoreF1.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Rouge/Rouge2ScoreF1.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['RougeScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'RougeScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22900e6c-f534-4f26-9b9a-2f2f00d2075d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Rouge/Rouge1ScoreF1.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Rouge/Rouge1ScoreF1.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['RougeScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'RougeScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6f0eba5-fbe8-4aa7-86d5-dd40fe6fdccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Rouge/RougeLScoreF1.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Rouge/RougeLScoreF1.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['RougeScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'RougeScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a7b3907-10e6-4da0-8ba9-e4112ca7c8c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Rouge/Rouge2ScoreF1.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Rouge/Rouge2ScoreF1.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['RougeScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'RougeScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6fc2e34-1b7d-4e17-be77-1cfcb8092f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Rouge/Rouge1ScoreF1.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Rouge/Rouge1ScoreF1.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['RougeScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'RougeScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16100f28-f0c9-4d72-b40d-4ec8618f6ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/BleuScore.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/BleuScore.csv\", 'a', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1bd7c4-d06a-453c-836a-1c39874edc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu1Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu1Score.csv\", 'a', newline='') as writefile:\n",
    "        fieldnames = ['Prediction', 'Labels', 'BleuScore']\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdacd7b6-59ff-4527-9429-777777419be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu2Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu2Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3ee382-e525-47fb-810b-c8b539c527a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu3Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu3Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e42b7bf-9398-4424-b64d-555c5028add2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu4Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Fine-tuned Model/Bleu/Bleu4Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "829d5c5c-0b0d-4b72-9c59-7f25ba8ab0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Bleu/Bleu4Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu/Bleu4Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fccd97f4-a639-4d93-a676-91f3dcefbcd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Bleu/Bleu3Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu/Bleu3Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15a9835a-a913-4aa5-b2ba-5fbc52518da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Bleu/Bleu2Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu/Bleu2Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecad8434-b2e8-4dcc-946d-a028a13351a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Bleu/Bleu1Score.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu/Bleu1Score.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7c8e4f8-cad5-4148-a3b0-1d6ead9448b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from rouge import Rouge\n",
    "\n",
    "with open(\"/Evaluation Scores/Base Model/Bleu/BleuScore.csv\", 'r') as file:\n",
    "    with open(\"/Evaluation Scores/Base Model/Bleu/BleuScore.csv\", 'a', newline='') as writefile:\n",
    "        writer = csv.DictWriter(writefile, fieldnames=fieldnames)\n",
    "        csvfile = csv.DictReader(file)\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in csvfile:\n",
    "            total += float(row['BleuScore'])\n",
    "            counter +=1\n",
    "        avg = total / counter\n",
    "        writer.writerow({'Prediction': '', 'Labels': \"Average\", 'BleuScore': avg})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine-Tuning Environment",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
