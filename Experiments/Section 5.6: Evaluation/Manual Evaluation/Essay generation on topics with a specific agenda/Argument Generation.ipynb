{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b04002a-44c3-4e13-8e8b-524ebcb33667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeda27e6b51404c8f78d9987870bec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LlamaTokenizer, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from adapters import AdapterConfig\n",
    "\n",
    "\n",
    "def generate_prompt(instruction: str, input_ctxt: str = None) -> str:\n",
    "    if input_ctxt:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with a quesion. Write a response that appropriately answers the question.\n",
    "        ### Instruction:\n",
    "        {instruction}\n",
    "        ### Input:\n",
    "        {input_ctxt}\n",
    "      \t### Response:\"\"\"\n",
    "    else:\n",
    "      return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    \t ### Instruction:\n",
    "      {instruction}\n",
    "       ### Response:\"\"\"\n",
    "\n",
    "\n",
    "def loadDataset():\n",
    "    test= pd.read_csv(\"/bigwork/nhwpshaa/Fine-tuning/Alpaca Native/data/test.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "    dataset = {\"test\": test }\n",
    "    return dataset\n",
    "\n",
    "def generateTrainingInstructions(dataset): #generate instructions specifically for training\n",
    "    data = {\"instruction\":[], \"input\":[], \"output\":[]}\n",
    "    for i, record in dataset.iterrows():\n",
    "        question = record['Question']\n",
    "        answer = record['Answer']\n",
    "        if record['Question Type'].lower() == \"yes/no question\": #if the question is of yes or no type, provide an individualized instruction\n",
    "            data[\"instruction\"].append(f\"Answer the following questions with 'Yes' or 'No'\")\n",
    "        else:\n",
    "            data[\"instruction\"].append(f\"Answer the following questions to the best of your knowledge. Answer the questions as briefly as possible using only one causal sentence.\")\n",
    "        data[\"output\"].append(answer)\n",
    "        data[\"input\"].append(question)\n",
    "    return data\n",
    "\n",
    "\n",
    "dataset = loadDataset()\n",
    "dataset = generateTrainingInstructions(dataset['test'])\n",
    "model = LlamaForCausalLM.from_pretrained(\"/bigwork/nhwpshaa/alpaca-native\")\n",
    "#adapter_config = AdapterConfig.load(\"pfeiffer\")\n",
    "model.load_adapter(\"/bigwork/nhwpshaa/Fine-tuned AlpacaNative/Jobs/30 epochs/LoRA config/\", adapter_name=\"adapter1\") #load the first adapter as adapter1\n",
    "model.load_adapter(\"/bigwork/nhwpshaa/Fine-tuned AlpacaNative/Jobs/30 epochs/LoRA config/secondOrder2\", adapter_name=\"adapter2\") #load the second adapter as adapter2\n",
    "model.enable_adapters() #enable both adapters\n",
    "model.eval() #change to evaluation mode for inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/bigwork/nhwpshaa/alpaca-native/\")\n",
    "labels= []\n",
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc66a4e-a8ea-4daf-8a53-62f3f3917258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadbe057-0741-4bf3-a6f2-dea0fcaa9b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Act as an expert debater and generate an essay of around 400 words arguing against the following topic. Structure the essay into well-organized paragraphs and provide evidence to support the claim\"\n",
    "input_ctxt = \"Schools should abolish homework\"\n",
    "sequences = pipeline(\n",
    "    generate_prompt(instruction, input_ctxt),\n",
    "    min_length=500,\n",
    "    max_length=900,\n",
    "    do_sample=True,\n",
    "    top_k=90,\n",
    "    num_return_sequences=4,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "777ef563-e35d-4f44-961d-68ae255189c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Software Env",
   "language": "python",
   "name": "newconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
