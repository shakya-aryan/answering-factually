{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055098b-6afa-48df-9b44-73e28f9afde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhwpshaa/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00421cd68803472dbf86324feca6d449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhwpshaa/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "/home/nhwpshaa/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/nhwpshaa/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "documents = []\n",
    "\n",
    "with open('/bigwork/nhwpshaa/BigDataset/firstOrderEffect.csv', 'r') as file: #load and read the file containing causal relationships\n",
    "    csvfile = csv.DictReader(file)\n",
    "    for row in csvfile:\n",
    "        content= f\"Cause:{row['cause']} Effect:{row['effect']}\" #form a sentence to provide as a content\n",
    "        documents.append(Document(page_content= content)) #create a document object and append all of it to the array\n",
    "        \n",
    "with open('/bigwork/nhwpshaa/BigDataset/cause.csv', 'r') as file: #load and read the file containing causal relationships\n",
    "    csvfile = csv.DictReader(file)\n",
    "    for row in csvfile:\n",
    "        content= f\"Cause:{row['cause']} Effect:{row['effect']}\" #form a sentence to provide as a content\n",
    "        documents.append(Document(page_content= content)) #load and read the file containing causal relationships\n",
    "        \n",
    "\n",
    "model = HuggingFaceEmbeddings(model_name='/bigwork/nhwpshaa/all-MiniLM-L6-v2') #initialize embeddings\n",
    "vectorDB = FAISS.from_documents(documents, model) #initialize vector database\n",
    "modelName = \"/bigwork/nhwpshaa/alpaca-native\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(modelName)\n",
    "llm = LlamaForCausalLM.from_pretrained(modelName, top_k = 50) #set the same settings as for other models\n",
    "pipelineModel = pipeline(\"text-generation\",\n",
    "    model=llm,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    min_length=500,\n",
    "    max_length=900,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    num_return_sequences=4,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "pipe = HuggingFacePipeline(pipeline=pipelineModel) \n",
    "prompt = PromptTemplate(\n",
    "    template=\"{context}\\nAct as an expert debater to generate an essay consisting of around 400 words for the following input topic. Assume only the 'con' stance. Segment the essay into well-structured pararaphs \\nTopic: {question}\\nAnswer:\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_llm(\n",
    "    llm=pipe,\n",
    "    retriever=vectorDB.as_retriever(), #initialize the vector databse as a retriever\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Crime\"\n",
    "response = chain({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Software Env",
   "language": "python",
   "name": "newconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
